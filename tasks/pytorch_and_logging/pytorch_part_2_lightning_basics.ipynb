{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "148d35eb",
   "metadata": {},
   "source": [
    "# [Pytorch-lightning](https://www.pytorchlightning.ai/)\n",
    "\n",
    "\n",
    "Составители туториала: [Артем Мухин](https://github.com/Banayaki)\n",
    "\n",
    "Данная библиотека является оберткой над библиотекой `pytorch`. Она позволяет существенным образом сократить количество написанного кода (который часто повторяется, например циклы обучения, перенос данных и модели на GPU), облегичить запуск экспериментов, а также автоматически искать гиперпараметры моделей. \n",
    "\n",
    "На главной странице сайта можно увидеть беглое сравнение кода написанного на `pytorch` и на `pytorch-lightning`.\n",
    "\n",
    "Далее сравним как изменится код, написанный нами ранее на чистом `pytorch` с кодом, написанным с использованием `pytorch-lighning` и убедимся в том, что данная утилита действительно позволяет избавиться от дублирования одного и того же кода раз за разом.\n",
    "\n",
    "В основе данной библиотеки лежат два класса `LightningModule`, служащий для конфигурирования модели и её параметров; и `Trainer`, служащий для запуска и контролирования обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29ebbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa9b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e52ea",
   "metadata": {},
   "source": [
    "## LightningModule\n",
    "Как уже было упомянуто, данный класс служит для создания и конфигурирования обучаемой модели, рассмотрим его использование в нашем случае"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ad3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMnistModel(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module, loss):\n",
    "        \"\"\"\n",
    "        :param model: непосредственно нейронная сеть\n",
    "        :param loss: функция ошибки\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Метод аналогичный методу forward() в torch.nn.Module\n",
    "        \"\"\"\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Метод в котором можно конфигурировать оптимизатор\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Также здесь можно настраивать и learning rate schedule \n",
    "        lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Наш новый тренировочный цикл.\n",
    "        Если посмотреть где вызывается данный метод, то под капотом в \n",
    "        pytorch_lightning написано ровно тоже самое, что мы писали вручную на pytorch:\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            loss = training_step(batch, batch_idx)  # А вот и наш training_step!\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "        \"\"\"\n",
    "        img, label = train_batch\n",
    "        preds = self.model(img)\n",
    "        loss = self.loss(preds, label)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss  # Важно не забыть вернуть loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Та часть тренировочного цикла, которая отвественна за валидацию.\n",
    "        Т.к. мы хотим вычислять метрики по всему валидационному набору данных, \n",
    "        непосредственное вычисление метрик мы напишем в следующем методе.\n",
    "        \n",
    "        Данный метод вызывается для каждого батча. Именно поэтому здесь\n",
    "        мы не можем вычислить метрики для всей эпохи (всего набора)\n",
    "        \"\"\"\n",
    "        imgs, labels = val_batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss(preds, labels)\n",
    "        return {'loss': loss, 'preds': preds, 'labels': labels}\n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        \"\"\"\n",
    "        validation_step_outputs - список возвращенных `validation_step` значений\n",
    "        \"\"\"\n",
    "        all_losses = torch.hstack(list(map(lambda item: item['loss'], validation_step_outputs)))\n",
    "        all_preds = torch.vstack(list(map(lambda item: item['preds'], validation_step_outputs)))\n",
    "        all_labels = torch.hstack(list(map(lambda item: item['labels'], validation_step_outputs)))\n",
    "        correct = (all_preds.argmax(dim=1) == all_labels).type(torch.float).sum().item()\n",
    "        acc = correct / all_labels.shape[0]\n",
    "        loss = torch.mean(all_losses)\n",
    "        self.log_dict({'Accuracy': acc, \"Val. Loss\": loss}, prog_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426df1fb",
   "metadata": {},
   "source": [
    "## Этап подготовки данных не изменился"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0defadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = T.Compose([\n",
    "    T.RandomRotation((-30, 30)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3799b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', \n",
    "    download=False, \n",
    "    train=True,\n",
    "    transform=augmentation\n",
    ")\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa3b1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet18(nn.Module):\n",
    "    def __init__(self, n_channels: int = 1, n_classes: int = 10):\n",
    "        super().__init__()\n",
    "        self.resnet = torchvision.models.resnet18(pretrained=False)\n",
    "        # Заменим первую свертку\n",
    "        self.resnet.conv1 = nn.Conv2d(in_channels=n_channels, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(in_features=self.resnet.fc.in_features, out_features=n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hx = self.resnet(x)\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79dff79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем снова нашу Resnet18 для FashionMNIST\n",
    "network = Resnet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce662423",
   "metadata": {},
   "source": [
    "## Создадим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ee9fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FashionMnistModel(network, torch.nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1527d59",
   "metadata": {},
   "source": [
    "## Применим класс Trainer для начала обучения\n",
    "\n",
    "У конструктора данного класса есть больше число разнообразных параметров, подробно они описаны в документации. Нас они сильно не интересуют. Обратим лишь внимание на параметр `gpus`, если данный параметр `!= 0` и `!=None`, тогда вычисления будут проводиться на 'GPU'. Заметьте, что больше нам не надо беспокоиться о переносе данных на GPU вручную.\n",
    "\n",
    "*Также есть параметры `devices` и `accelerator`, которые также позволяют выбирать где будут выполняться вычисления.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70dfdbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=10,\n",
    "    check_val_every_n_epoch=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bd203",
   "metadata": {},
   "source": [
    "Обучение запускается вызовом метода `fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110f1e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | Resnet18         | 11.2 M\n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.701    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 59/118 [00:05<00:05, 11.35it/s, loss=0.455, v_num=39]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  52%|█████▏    | 61/118 [00:05<00:05, 10.74it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  54%|█████▍    | 64/118 [00:05<00:04, 10.99it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  58%|█████▊    | 68/118 [00:05<00:04, 11.39it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  61%|██████    | 72/118 [00:06<00:03, 11.77it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  64%|██████▍   | 76/118 [00:06<00:03, 12.13it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  68%|██████▊   | 80/118 [00:06<00:03, 12.48it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  71%|███████   | 84/118 [00:06<00:02, 12.81it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  75%|███████▍  | 88/118 [00:06<00:02, 13.11it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  78%|███████▊  | 92/118 [00:06<00:01, 13.42it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  81%|████████▏ | 96/118 [00:07<00:01, 13.71it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  85%|████████▍ | 100/118 [00:07<00:01, 13.99it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  88%|████████▊ | 104/118 [00:07<00:00, 14.25it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  92%|█████████▏| 108/118 [00:07<00:00, 14.51it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  95%|█████████▍| 112/118 [00:07<00:00, 14.75it/s, loss=0.455, v_num=39]\n",
      "Epoch 0:  98%|█████████▊| 116/118 [00:07<00:00, 14.99it/s, loss=0.455, v_num=39]\n",
      "Epoch 0: 100%|██████████| 118/118 [00:07<00:00, 15.02it/s, loss=0.455, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  51%|█████     | 60/118 [00:05<00:04, 11.86it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/59 [00:00<00:27,  2.13it/s]\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 64/118 [00:05<00:04, 11.28it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  58%|█████▊    | 68/118 [00:05<00:04, 11.68it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  61%|██████    | 72/118 [00:05<00:03, 12.07it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  64%|██████▍   | 76/118 [00:06<00:03, 12.44it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  68%|██████▊   | 80/118 [00:06<00:02, 12.79it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  71%|███████   | 84/118 [00:06<00:02, 13.12it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  75%|███████▍  | 88/118 [00:06<00:02, 13.44it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  78%|███████▊  | 92/118 [00:06<00:01, 13.75it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  81%|████████▏ | 96/118 [00:06<00:01, 14.04it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  85%|████████▍ | 100/118 [00:06<00:01, 14.33it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  88%|████████▊ | 104/118 [00:07<00:00, 14.59it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  92%|█████████▏| 108/118 [00:07<00:00, 14.85it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  95%|█████████▍| 112/118 [00:07<00:00, 15.10it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1:  98%|█████████▊| 116/118 [00:07<00:00, 15.34it/s, loss=0.399, v_num=39, Accuracy=0.832, Val. Loss=0.460]\n",
      "Epoch 1: 100%|██████████| 118/118 [00:07<00:00, 15.36it/s, loss=0.399, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  51%|█████     | 60/118 [00:05<00:04, 11.83it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/59 [00:00<00:27,  2.13it/s]\u001b[A\n",
      "Epoch 2:  54%|█████▍    | 64/118 [00:05<00:04, 11.22it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  58%|█████▊    | 68/118 [00:05<00:04, 11.58it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  61%|██████    | 72/118 [00:06<00:03, 11.92it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  64%|██████▍   | 76/118 [00:06<00:03, 12.28it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  68%|██████▊   | 80/118 [00:06<00:03, 12.63it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  71%|███████   | 84/118 [00:06<00:02, 12.96it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  75%|███████▍  | 88/118 [00:06<00:02, 13.28it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  78%|███████▊  | 92/118 [00:06<00:01, 13.58it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  81%|████████▏ | 96/118 [00:06<00:01, 13.87it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  85%|████████▍ | 100/118 [00:07<00:01, 14.16it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  88%|████████▊ | 104/118 [00:07<00:00, 14.42it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  92%|█████████▏| 108/118 [00:07<00:00, 14.68it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  95%|█████████▍| 112/118 [00:07<00:00, 14.93it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2:  98%|█████████▊| 116/118 [00:07<00:00, 15.17it/s, loss=0.355, v_num=39, Accuracy=0.843, Val. Loss=0.420]\n",
      "Epoch 2: 100%|██████████| 118/118 [00:07<00:00, 15.20it/s, loss=0.355, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  51%|█████     | 60/118 [00:05<00:04, 11.77it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/59 [00:00<00:28,  2.02it/s]\u001b[A\n",
      "Epoch 3:  54%|█████▍    | 64/118 [00:05<00:04, 11.15it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  58%|█████▊    | 68/118 [00:05<00:04, 11.55it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  61%|██████    | 72/118 [00:06<00:03, 11.94it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  64%|██████▍   | 76/118 [00:06<00:03, 12.30it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  68%|██████▊   | 80/118 [00:06<00:03, 12.65it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  71%|███████   | 84/118 [00:06<00:02, 12.95it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  75%|███████▍  | 88/118 [00:06<00:02, 13.27it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  78%|███████▊  | 92/118 [00:06<00:01, 13.57it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  81%|████████▏ | 96/118 [00:06<00:01, 13.86it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  85%|████████▍ | 100/118 [00:07<00:01, 14.13it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  88%|████████▊ | 104/118 [00:07<00:00, 14.40it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  92%|█████████▏| 108/118 [00:07<00:00, 14.66it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3:  95%|█████████▍| 112/118 [00:07<00:00, 14.90it/s, loss=0.333, v_num=39, Accuracy=0.858, Val. Loss=0.382]\n",
      "Epoch 3: 100%|██████████| 118/118 [00:07<00:00, 15.20it/s, loss=0.333, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  51%|█████     | 60/118 [00:05<00:04, 11.72it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/59 [00:00<00:27,  2.12it/s]\u001b[A\n",
      "Epoch 4:  54%|█████▍    | 64/118 [00:05<00:04, 11.15it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  58%|█████▊    | 68/118 [00:05<00:04, 11.55it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  61%|██████    | 72/118 [00:06<00:03, 11.93it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  64%|██████▍   | 76/118 [00:06<00:03, 12.29it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  68%|██████▊   | 80/118 [00:06<00:03, 12.64it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  71%|███████   | 84/118 [00:06<00:02, 12.97it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  75%|███████▍  | 88/118 [00:06<00:02, 13.29it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  78%|███████▊  | 92/118 [00:06<00:01, 13.58it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  81%|████████▏ | 96/118 [00:06<00:01, 13.87it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  85%|████████▍ | 100/118 [00:07<00:01, 14.15it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  88%|████████▊ | 104/118 [00:07<00:00, 14.41it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  92%|█████████▏| 108/118 [00:07<00:00, 14.67it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  95%|█████████▍| 112/118 [00:07<00:00, 14.92it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4:  98%|█████████▊| 116/118 [00:07<00:00, 15.15it/s, loss=0.317, v_num=39, Accuracy=0.863, Val. Loss=0.364]\n",
      "Epoch 4: 100%|██████████| 118/118 [00:07<00:00, 15.18it/s, loss=0.317, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  51%|█████     | 60/118 [00:05<00:04, 11.65it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/59 [00:00<00:26,  2.15it/s]\u001b[A\n",
      "Epoch 5:  54%|█████▍    | 64/118 [00:05<00:04, 11.10it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  58%|█████▊    | 68/118 [00:05<00:04, 11.50it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  61%|██████    | 72/118 [00:06<00:03, 11.89it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  64%|██████▍   | 76/118 [00:06<00:03, 12.25it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  68%|██████▊   | 80/118 [00:06<00:03, 12.60it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  71%|███████   | 84/118 [00:06<00:02, 12.94it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  75%|███████▍  | 88/118 [00:06<00:02, 13.21it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  78%|███████▊  | 92/118 [00:06<00:01, 13.48it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  81%|████████▏ | 96/118 [00:06<00:01, 13.72it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  85%|████████▍ | 100/118 [00:07<00:01, 13.96it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  88%|████████▊ | 104/118 [00:07<00:00, 14.19it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  92%|█████████▏| 108/118 [00:07<00:00, 14.41it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  95%|█████████▍| 112/118 [00:07<00:00, 14.55it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5:  98%|█████████▊| 116/118 [00:07<00:00, 14.82it/s, loss=0.307, v_num=39, Accuracy=0.879, Val. Loss=0.320]\n",
      "Epoch 5: 100%|██████████| 118/118 [00:07<00:00, 14.85it/s, loss=0.307, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  51%|█████     | 60/118 [00:05<00:04, 11.63it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/59 [00:00<00:28,  2.04it/s]\u001b[A\n",
      "Epoch 6:  54%|█████▍    | 64/118 [00:05<00:04, 11.04it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  58%|█████▊    | 68/118 [00:05<00:04, 11.44it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  61%|██████    | 72/118 [00:06<00:03, 11.82it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  64%|██████▍   | 76/118 [00:06<00:03, 12.18it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  68%|██████▊   | 80/118 [00:06<00:03, 12.52it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  71%|███████   | 84/118 [00:06<00:02, 12.85it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  75%|███████▍  | 88/118 [00:06<00:02, 13.16it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  78%|███████▊  | 92/118 [00:06<00:01, 13.47it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  81%|████████▏ | 96/118 [00:06<00:01, 13.76it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  85%|████████▍ | 100/118 [00:07<00:01, 14.03it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  88%|████████▊ | 104/118 [00:07<00:00, 14.30it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  92%|█████████▏| 108/118 [00:07<00:00, 14.55it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  95%|█████████▍| 112/118 [00:07<00:00, 14.80it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6:  98%|█████████▊| 116/118 [00:07<00:00, 15.04it/s, loss=0.292, v_num=39, Accuracy=0.888, Val. Loss=0.298]\n",
      "Epoch 6: 100%|██████████| 118/118 [00:07<00:00, 15.07it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  51%|█████     | 60/118 [00:05<00:04, 11.64it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/59 [00:00<00:27,  2.11it/s]\u001b[A\n",
      "Epoch 7:  54%|█████▍    | 64/118 [00:05<00:04, 11.08it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  58%|█████▊    | 68/118 [00:05<00:04, 11.49it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  61%|██████    | 72/118 [00:06<00:03, 11.87it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  64%|██████▍   | 76/118 [00:06<00:03, 12.23it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  68%|██████▊   | 80/118 [00:06<00:03, 12.58it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  71%|███████   | 84/118 [00:06<00:02, 12.90it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  75%|███████▍  | 88/118 [00:06<00:02, 13.22it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  78%|███████▊  | 92/118 [00:06<00:01, 13.52it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  81%|████████▏ | 96/118 [00:06<00:01, 13.82it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  85%|████████▍ | 100/118 [00:07<00:01, 14.09it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  88%|████████▊ | 104/118 [00:07<00:00, 14.36it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  92%|█████████▏| 108/118 [00:07<00:00, 14.61it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  95%|█████████▍| 112/118 [00:07<00:00, 14.86it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7:  98%|█████████▊| 116/118 [00:07<00:00, 15.09it/s, loss=0.292, v_num=39, Accuracy=0.890, Val. Loss=0.292]\n",
      "Epoch 7: 100%|██████████| 118/118 [00:07<00:00, 15.13it/s, loss=0.292, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  51%|█████     | 60/118 [00:05<00:04, 11.62it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/59 [00:00<00:26,  2.17it/s]\u001b[A\n",
      "Epoch 8:  54%|█████▍    | 64/118 [00:05<00:04, 11.09it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  58%|█████▊    | 68/118 [00:05<00:04, 11.49it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  61%|██████    | 72/118 [00:06<00:03, 11.88it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  64%|██████▍   | 76/118 [00:06<00:03, 12.24it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  68%|██████▊   | 80/118 [00:06<00:03, 12.59it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  71%|███████   | 84/118 [00:06<00:02, 12.93it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  75%|███████▍  | 88/118 [00:06<00:02, 13.25it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  78%|███████▊  | 92/118 [00:06<00:01, 13.54it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  81%|████████▏ | 96/118 [00:06<00:01, 13.79it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  85%|████████▍ | 100/118 [00:07<00:01, 14.03it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  88%|████████▊ | 104/118 [00:07<00:00, 14.25it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  92%|█████████▏| 108/118 [00:07<00:00, 14.47it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  95%|█████████▍| 112/118 [00:07<00:00, 14.68it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8:  98%|█████████▊| 116/118 [00:07<00:00, 14.88it/s, loss=0.287, v_num=39, Accuracy=0.885, Val. Loss=0.303]\n",
      "Epoch 8: 100%|██████████| 118/118 [00:07<00:00, 14.82it/s, loss=0.287, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  51%|█████     | 60/118 [00:05<00:04, 11.63it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/59 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▏         | 1/59 [00:00<00:27,  2.12it/s]\u001b[A\n",
      "Epoch 9:  54%|█████▍    | 64/118 [00:05<00:04, 11.09it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  58%|█████▊    | 68/118 [00:05<00:04, 11.50it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  61%|██████    | 72/118 [00:06<00:03, 11.87it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  64%|██████▍   | 76/118 [00:06<00:03, 12.24it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  68%|██████▊   | 80/118 [00:06<00:03, 12.58it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  71%|███████   | 84/118 [00:06<00:02, 12.91it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  75%|███████▍  | 88/118 [00:06<00:02, 13.23it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  78%|███████▊  | 92/118 [00:06<00:01, 13.54it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  81%|████████▏ | 96/118 [00:06<00:01, 13.83it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  85%|████████▍ | 100/118 [00:07<00:01, 14.11it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  88%|████████▊ | 104/118 [00:07<00:00, 14.38it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  92%|█████████▏| 108/118 [00:07<00:00, 14.65it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  95%|█████████▍| 112/118 [00:07<00:00, 14.89it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9:  98%|█████████▊| 116/118 [00:07<00:00, 15.13it/s, loss=0.262, v_num=39, Accuracy=0.883, Val. Loss=0.307]\n",
      "Epoch 9: 100%|██████████| 118/118 [00:07<00:00, 15.16it/s, loss=0.262, v_num=39, Accuracy=0.903, Val. Loss=0.254]\n",
      "Epoch 9: 100%|██████████| 118/118 [00:08<00:00, 14.69it/s, loss=0.262, v_num=39, Accuracy=0.903, Val. Loss=0.254]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataloader_train, dataloader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bee027",
   "metadata": {},
   "source": [
    "У `Trainer` также есть методы для проведения валидации и тестирования модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec79ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:  95%|█████████▍| 56/59 [00:02<00:00, 27.28it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'Accuracy': 0.9046000242233276, 'Val. Loss': 0.253422349691391}\n",
      "--------------------------------------------------------------------------------\n",
      "Validating: 100%|██████████| 59/59 [00:02<00:00, 22.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Accuracy': 0.9046000242233276, 'Val. Loss': 0.253422349691391}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, dataloader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f6fa6",
   "metadata": {},
   "source": [
    "# Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f0e84",
   "metadata": {},
   "source": [
    "Обратите внимание, на то, как `pytorch-lightning` организовал код и уменьшил необходимость дублицировать вновь и вновь один и тот же код. Это далеко не все возможности, предоставляемые данной библиотекой. Она также позволяет легко переключать режим точности (FP32->FP16), добавлять логирование (в том числе с помощью упомянутых comet.ml и т.д.), проводить оптимизацию и профилирование нейронной сети и многое другое. Все возможности, которыми обладает данная библиотека описаны в её докумментации. Также в документации разработчики добавили небольшие видео-ролики, в которых коротко рассказывают о том или ином параметре/методе. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-resource",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
